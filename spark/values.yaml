# spark/values.yaml
# Spark Operator configuration

# Image configuration
image:
  repository: kubeflow/spark-operator
  tag: "v1.1.27"
  pullPolicy: IfNotPresent

# Controller configuration
controller:
  replicas: 1
  logLevel: 2  # INFO level
  
  # Resource limits
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Webhook configuration
webhook:
  enable: true
  port: 8080
  
  # Resource limits for webhook
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Service Account configurations
serviceAccounts:
  # Operator service account
  sparkoperator:
    create: true
    name: spark-operator
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::399883341639:role/data-platform-spark-s3-access"
  
  # Spark applications service account
  spark:
    create: true
    name: spark
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::399883341639:role/data-platform-spark-s3-access"

# Spark job configuration
spark:
  # Namespaces where Spark applications can be deployed
  jobNamespaces:
    - "spark"
    - "data-processing"
    - "default"

# RBAC configuration
rbac:
  create: true
  createClusterRole: true
  createRole: true

# Leader election (for HA)
leaderElection:
  lockName: "spark-operator-lock"
  lockNamespace: "spark"

# Metrics and monitoring
metrics:
  enable: true
  port: 10254
  portName: metrics
  endpoint: /metrics
  
  # Prometheus configuration
  serviceMonitor:
    enable: true
    labels:
      prometheus: kube-prometheus-stack
    jobStartLatencyBuckets:
      - 30
      - 60
      - 120
      - 300
      - 600
      - 1200

# Node selection - use system nodes for operator
nodeSelector:
  node-group: system

tolerations: []
affinity: {}

# Security context
securityContext:
  runAsUser: 65534
  runAsGroup: 65534
  runAsNonRoot: true

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65534
  fsGroup: 65534

# Additional environment variables
env:
  - name: AWS_REGION
    value: eu-west-3
  - name: SPARK_S3_BUCKET
    value: data-platform-data-lake-s3d92803

# Volume mounts for additional configurations
volumeMounts: []
volumes: []

# Istio configuration (if using service mesh)
istio:
  enabled: false

# Resource quotas (optional)
resourceQuotaEnforcement:
  enable: false

# Batch scheduler integration (if using Volcano, etc.)
batchScheduler:
  enable: false

# Spark configuration defaults for applications
sparkJobNamespace: spark
sparkVersion: "3.3.0"
sparkImagePullPolicy: IfNotPresent
sparkImageRegistry: ""

# Default Spark application settings
defaultSparkConf:
  "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
  "spark.hadoop.fs.s3a.endpoint": "s3.eu-west-3.amazonaws.com"
  "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
  "spark.kubernetes.authenticate.driver.serviceAccountName": "spark"
  "spark.kubernetes.authenticate.executor.serviceAccountName": "spark"
  "spark.eventLog.enabled": "true"
  "spark.eventLog.dir": "s3a://data-platform-logs-s3d92803/spark-events/"