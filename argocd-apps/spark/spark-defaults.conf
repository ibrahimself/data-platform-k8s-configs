# argocd-apps/spark/spark-defaults.conf
# Spark configuration for jobs running on Kubernetes

# History Server configuration
spark.eventLog.enabled=true
spark.eventLog.dir=file:///tmp/spark-events
spark.history.fs.logDirectory=file:///tmp/spark-events

# Kubernetes configuration
spark.master=k8s://https://kubernetes.default.svc
spark.kubernetes.namespace=spark
spark.kubernetes.authenticate.driver.serviceAccountName=spark
spark.kubernetes.authenticate.executor.serviceAccountName=spark

# Container image
spark.kubernetes.container.image=apache/spark:3.5.3
spark.kubernetes.container.image.pullPolicy=IfNotPresent

# Resource allocation
spark.executor.instances=2
spark.executor.memory=2g
spark.executor.cores=1
spark.driver.memory=2g
spark.driver.cores=1

# Dynamic allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.shuffleTracking.enabled=true

# Monitoring
spark.ui.prometheus.enabled=true
spark.executor.processTreeMetrics.enabled=true
spark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus

# S3 configuration (if using S3 for data)
# spark.hadoop.fs.s3a.endpoint=s3.amazonaws.com
# spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY
# spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY
# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem