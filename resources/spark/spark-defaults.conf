# Spark configuration for jobs running on Kubernetes

# History Server configuration
spark.eventLog.enabled=true
spark.eventLog.dir=file:///tmp/spark-events
spark.history.fs.logDirectory=file:///tmp/spark-events

# Kubernetes configuration
spark.master=k8s://https://kubernetes.default.svc
spark.kubernetes.namespace=spark
spark.kubernetes.authenticate.driver.serviceAccountName=spark
spark.kubernetes.authenticate.executor.serviceAccountName=spark

# Container image
spark.kubernetes.container.image=apache/spark:3.5.3
spark.kubernetes.container.image.pullPolicy=IfNotPresent

# Resource allocation defaults
spark.executor.instances=2
spark.executor.memory=2g
spark.executor.cores=1
spark.driver.memory=2g
spark.driver.cores=1

# Dynamic allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.shuffleTracking.enabled=true
spark.dynamicAllocation.shuffleTracking.timeout=300s

# Kubernetes executor settings
spark.kubernetes.executor.deleteOnTermination=true
spark.kubernetes.executor.request.cores=1
spark.kubernetes.executor.limit.cores=2

# UI and monitoring
spark.ui.enabled=true
spark.ui.port=4040
spark.ui.reverseProxy=true
spark.ui.reverseProxyUrl=/spark
spark.ui.prometheus.enabled=true
spark.executor.processTreeMetrics.enabled=true
spark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus

# SQL configuration
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true

# S3 configuration (for S3/MinIO data access)
# spark.hadoop.fs.s3a.endpoint=http://minio.minio.svc.cluster.local:9000
# spark.hadoop.fs.s3a.access.key=minio
# spark.hadoop.fs.s3a.secret.key=minio123
# spark.hadoop.fs.s3a.path.style.access=true
# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# PostgreSQL/JDBC configuration
spark.jars.packages=org.postgresql:postgresql:42.7.1,org.apache.hadoop:hadoop-aws:3.3.4

# Network configuration
spark.kubernetes.driver.service.enabled=true
spark.kubernetes.driver.service.port=4040

# Security
spark.authenticate=false
spark.network.crypto.enabled=false

# Shuffle service (if using external shuffle)
# spark.shuffle.service.enabled=true
# spark.kubernetes.shuffle.namespace=spark
# spark.kubernetes.shuffle.labels=app=spark-shuffle