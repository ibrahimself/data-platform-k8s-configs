# Apache Airflow configuration for Data Platform

# Airflow image
images:
  airflow:
    repository: apache/airflow
    tag: "2.10.5"
    pullPolicy: IfNotPresent

# Airflow executor - KubernetesExecutor for scalability
executor: "KubernetesExecutor"

# Environment variables
env:
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: "False"
  - name: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
    value: "True"
  - name: AIRFLOW__CORE__EXECUTOR
    value: "KubernetesExecutor"
  - name: AIRFLOW__KUBERNETES__NAMESPACE
    value: "airflow"
  - name: AIRFLOW__KUBERNETES__WORKER_SERVICE_ACCOUNT_NAME
    value: "airflow-worker"
  - name: AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY
    value: "apache/airflow"
  - name: AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG
    value: "2.10.5"
  - name: AIRFLOW__WEBSERVER__BASE_URL
    value: "http://localhost:8080/airflow"

# Airflow configuration overrides
config:
  api:
    auth_backend: airflow.api.auth.backend.basic_auth
  core:
    dags_are_paused_at_creation: "True"
    load_default_connections: "False"
    colored_console_log: "False"
  kubernetes:
    delete_worker_pods: "True"
    delete_worker_pods_on_failure: "False"
    worker_pods_creation_batch_size: "10"
  webserver:
    expose_config: "True"
    rbac: "True"
    base_url: "http://localhost:8080/airflow"
  email:
    email_backend: airflow.utils.email.send_email_smtp
    smtp_host: localhost
    smtp_starttls: "False"
    smtp_ssl: "False"
    smtp_port: "25"

# Database - Use external PostgreSQL
data:
  metadataSecretName: airflow-metadata-secret
  resultBackendSecretName: airflow-result-backend-secret

# PostgreSQL - Disabled, using external
postgresql:
  enabled: false

# Webserver configuration
webserver:
  replicas: 1
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  service:
    type: ClusterIP
    ports:
      - name: airflow-ui
        port: 8080
  baseUrl: "http://localhost:8080/airflow"
  # Default admin user
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: Admin
    lastName: User
    password: admin
  # Secret name for webserver secret key
  webserverSecretKeySecretName: airflow-webserver-secret

# Scheduler configuration
scheduler:
  replicas: 1
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  # Enable log cleanup
  logGroomerSidecar:
    enabled: true
    retentionDays: 30

# Workers configuration (for KubernetesExecutor)
workers:
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

# Triggerer configuration
triggerer:
  enabled: true
  replicas: 1
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi

# Redis - Disabled for KubernetesExecutor
redis:
  enabled: false

# Flower - Disabled for KubernetesExecutor
flower:
  enabled: false

# DAGs configuration
dags:
  persistence:
    enabled: true
    size: 10Gi
    storageClassName: gp2  # Update based on your storage class
    accessMode: ReadWriteMany
  gitSync:
    enabled: false
    # Enable if you want to sync DAGs from git
    # repo: https://github.com/your-org/airflow-dags.git
    # branch: main
    # rev: HEAD
    # depth: 1
    # maxFailures: 0
    # subPath: dags
    # sshKeySecret: airflow-git-ssh-secret
    # wait: 60
    # containerName: git-sync
    # uid: 65533

# Logs configuration
logs:
  persistence:
    enabled: true
    size: 20Gi
    storageClassName: gp2  # Update based on your storage class
    existingClaim: null

# Fernet key configuration
fernetKey: fk6Rc2F9PvDkGPKpbGq6ysKmluXCHzpMwJitx-JB8JU=  # CHANGE THIS IN PRODUCTION!
fernetKeySecretName: airflow-fernet-key

# ServiceAccount
serviceAccount:
  create: true
  name: airflow
  annotations: {}

# ServiceAccount for Airflow workers
workers:
  serviceAccount:
    create: true
    name: airflow-worker

# RBAC
rbac:
  create: true
  createSCCRoleBinding: false

# Security context
securityContext:
  runAsUser: 50000
  runAsGroup: 0
  fsGroup: 0

# Pod security context (legacy - for older chart versions)
podSecurityContext:
  runAsUser: 50000
  runAsGroup: 0
  fsGroup: 0

# Node selector
nodeSelector: {}
  # node-group: compute

# Monitoring
metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    labels:
      prometheus: kube-prometheus-stack

# StatsD for metrics
statsd:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Cleanup
cleanup:
  enabled: true
  schedule: "0 0 * * *"  # Daily at midnight

# Extra pip packages to install
extraPipPackages:
  - apache-airflow-providers-kubernetes>=8.0.0
  - apache-airflow-providers-amazon>=8.0.0
  - apache-airflow-providers-postgres>=5.0.0
  - apache-airflow-providers-http>=4.0.0
  - apache-airflow-providers-spark>=4.0.0
  - pandas>=2.0.0
  - numpy>=1.24.0

# Extra volume mounts for DAGs
extraVolumeMounts:
  - name: dags
    mountPath: /opt/airflow/dags
    readOnly: false

extraVolumes:
  - name: dags
    persistentVolumeClaim:
      claimName: airflow-dags

# Ingress configuration (disabled - we manage it separately)
ingress:
  enabled: false

# Create user job
createUserJob:
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync
    "argocd.argoproj.io/hook-delete-policy": HookSucceeded

# Migrate database job
migrateDatabaseJob:
  enabled: true
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync
    "argocd.argoproj.io/hook-delete-policy": HookSucceeded